{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SCPScrapingJokes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup, SoupStrainer\n",
        "import httplib2\n",
        "import re\n",
        "import pandas as pd\n",
        "from itertools import chain\n",
        "import time \n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "yuvlEggrw2N7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting up dataframe with columns required\n",
        "joke_df = pd.DataFrame(columns=['code', 'title', 'text', 'image captions', 'rating', 'state', 'tags',  'link'], index=None)"
      ],
      "metadata": {
        "id": "IaESIR7f5MGs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "http = httplib2.Http()"
      ],
      "metadata": {
        "id": "KA-qlOac-t3P"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scraping joke scp pages based on directory of all joke scps\n",
        "\n",
        "# get all links first then loop through, maybe preserve as tag to then get code from there more easily etc.\n",
        "from itertools import chain\n",
        "\n",
        "status, directory_response = http.request('https://scp-wiki.wikidot.com/joke-scps')\n",
        "\n",
        "directory_soup = BeautifulSoup(directory_response, 'html.parser')\n",
        "\n",
        "links = [ i.find_all('a', href=True) for i in directory_soup.find_all('div', {'class': 'content-panel'})]\n",
        "# remove theme links/not actual links\n",
        "links = list(chain.from_iterable(links))\n",
        "\n",
        "links = [i for i in links if 'theme:' not in i['href'] and i.text[-1] != ':' and i.text not in ['Home Page', 'Bearitage Collection']]\n",
        "print(links)"
      ],
      "metadata": {
        "id": "6Y3_YEbgMT6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(links))"
      ],
      "metadata": {
        "id": "x9GwUw11OPln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for c, link in enumerate(links):\n",
        "  # timeout to limit load on the site (based on api requests per minute limit)\n",
        "  if c % 240 == 239:\n",
        "    print(c)\n",
        "    time.sleep(60)\n",
        "\n",
        "  # requesting page this tag refers to\n",
        "  url = 'https://scp-wiki.wikidot.com/' + link['href']\n",
        "  status, response = http.request(url)\n",
        "  soup = BeautifulSoup(response, 'html.parser')\n",
        "\n",
        "  # getting title from directory page\n",
        "  if ' - ' in link.parent.text:\n",
        "    title = link.parent.text.split(' - ')[-1]\n",
        "    code = link.parent.text.split(' - ')[0]\n",
        "  else:\n",
        "    title = link.text \n",
        "    # could also remove 'codename:' part from some of them but probably something for later in data processing conditional on how want to use\n",
        "    code = None\n",
        "  \n",
        "  captions = []\n",
        "  state = None\n",
        "  rating = None\n",
        "\n",
        "  # if soup.find(lambda x: x.name==\"a\" and \"clearance credentials\" in x.text): \n",
        "  # checking if there's any restricted by clearance - there aren't so no need to handle\n",
        "  #   print('RESTRICTED!!!!')\n",
        "\n",
        "  if soup.find('div', {'id': 'u-adult-warning'}) != None:\n",
        "    # print(\"ADULT!!!!\") # initial check to see if this set contains any adult restricted articles - which it does so need to handle\n",
        "    url = 'https://scp-wiki.wikidot.com/adult:{}/noredirect/true'.format(link['href'][1:])\n",
        "\n",
        "    status, adult_response = http.request(url)\n",
        "    soup_adult = BeautifulSoup(adult_response)\n",
        "\n",
        "    state = \"age restricted\"\n",
        "\n",
        "    rating_tag = soup_adult.find(class_ = 'number prw54353')\n",
        "\n",
        "    if rating_tag != None:\n",
        "      rating = rating_tag.text  \n",
        "\n",
        "    text = [i for i in soup_adult.find(id = 'page-content').find_all('p')]\n",
        "\n",
        "    tags_raw = soup_adult.find('div', {'class': 'page-tags'}).find('span').find_all('a')\n",
        "    tags = \" \".join([i.text for i in tags_raw]).replace(\",\", \"\")\n",
        "  else:\n",
        "    # getting text for the scp within the main content div\n",
        "    text = [i for i in soup.find(id = 'page-content').find_all('p')]\n",
        "    \n",
        "  # p tags include image captions which are excluded as below, may be other similar cases left in\n",
        "  for i in text:\n",
        "    try:\n",
        "      if i.parent.attrs['class'][0] == 'scp-image-caption':\n",
        "        captions.append(i.text)\n",
        "        text.remove(i)\n",
        "    except:\n",
        "      pass\n",
        "      \n",
        "  # getting text from result set\n",
        "  text = [i.text for i in text]\n",
        "\n",
        "  # getting the rating\n",
        "  if rating == None: # accounting for ones that dont have ratings on main page, mostly due to adult content notice page first\n",
        "    rating = soup.find(class_ = 'number prw54353')\n",
        "    if rating == None:\n",
        "      pass\n",
        "    else:\n",
        "      rating = rating.text\n",
        "\n",
        "  # getting tags\n",
        "  if not state:\n",
        "    tags_raw = soup.find('div', {'class': 'page-tags'}).find('span').find_all('a') if soup.find('div', {'class': 'page-tags'}) != None else None\n",
        "    tags = \" \".join([i.text for i in tags_raw]).replace(\",\", \"\") if tags_raw != None else None\n",
        "  else:\n",
        "    # just in case useful tags spread across the two pages here getting from restricted notif page too and adding\n",
        "    tags_raw = soup.find('div', {'class': 'page-tags'}).find('span').find_all('a')\n",
        "    tags = \" \".join(list(set([i.text for i in tags_raw] + tags.split()))).replace(\",\", \"\")\n",
        "  \n",
        "  # quotes to preserve commas and not cause csv issues - joining up p tags with newlines\n",
        "  text = \"\\\"{}\\\"\".format(\" \\n \".join([i.strip() for i in text if len(i.strip()) > 0])) if len(text) > 0 else None \n",
        "\n",
        "  captions = \"\\\"{}\\\"\".format(\" \\n \".join(i for i in captions)) if len(captions) > 0 else None\n",
        "\n",
        "  # setting status - age restricted will already have been assigned\n",
        "  if state:\n",
        "    pass\n",
        "  elif text == None:\n",
        "    state = \"missing\"\n",
        "  elif text and \"This page may have been moved or deleted\" in text:\n",
        "    state = \"deleted\"\n",
        "  elif \"[blocked]\" in title.lower():\n",
        "    state = \"blocked\"\n",
        "  else:\n",
        "    state = \"active\" # this will catch any anomalies too\n",
        "\n",
        "  info = [code, \"\\\"{}\\\"\".format(title), text, captions, rating, state, tags, url]\n",
        "  joke_df.loc[len(joke_df)] = info\n",
        "\n",
        "joke_df.to_csv('scpjoke.csv', index=False) \n",
        "\n",
        "# auto download after saving to file - with timeout to try and ensure the file will be available/ complete\n",
        "time.sleep(100)\n",
        "files.download('scpjoke.csv') "
      ],
      "metadata": {
        "id": "-ul7CD-y-rGZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}